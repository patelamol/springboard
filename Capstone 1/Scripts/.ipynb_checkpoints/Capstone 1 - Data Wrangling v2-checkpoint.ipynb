{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone 1 Project: Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Focus of exploratory data analysis will be;\n",
    "- Missing data (Null values)\n",
    "- Invalid data\n",
    "- Duplicates\n",
    "- Inconsistent column naming\n",
    "- Untidiness\n",
    "- Any further processing of data to make it meaningful\n",
    "- Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The crop information is available with variety of different data points but we will focus mainly on Yield and how it is affected by the Arces harvested, climate and labor avalability. \n",
    "<p><b>Acres Harvested</b>: Total strech of land from which the crop is harvested measured in arces. Where 1 acres is 43560 sq. ft.</p>\n",
    "<p><b>Production</b>: The total amount of crop produce by weight measured in Bushel (imperical unit).<p>\n",
    "<p><b>Yield</b>: Yield is the output of the land per acres, so it is production per acres (Production/Acres Harvested)</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'Data/USDA_Data_v2.0.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-580b5cbd4c7c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdata_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'Data/USDA_Data_v2.0.xlsx'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdata_xl_sheets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExcelFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msheet_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mcorn_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msheet_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Corn'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Year'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'State'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'County'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Data Item'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Value'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mbarley_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msheet_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Barley'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Year'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'State'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'County'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Data Item'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Value'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mpeanuts_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msheet_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Peanuts'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Year'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'State'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'County'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Data Item'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Value'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda2\\lib\\site-packages\\pandas\\io\\excel.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, io, **kwds)\u001b[0m\n\u001b[0;32m    392\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxlrd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen_workbook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_contents\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_io\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 394\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxlrd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen_workbook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_io\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    395\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m             raise ValueError('Must explicitly set engine if not passing in'\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda2\\lib\\site-packages\\xlrd\\__init__.py\u001b[0m in \u001b[0;36mopen_workbook\u001b[1;34m(filename, logfile, verbosity, use_mmap, file_contents, encoding_override, formatting_info, on_demand, ragged_rows)\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[0mpeek\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfile_contents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mpeeksz\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m             \u001b[0mpeek\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpeeksz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mpeek\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34mb\"PK\\x03\\x04\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# a ZIP file\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'Data/USDA_Data_v2.0.xlsx'"
     ]
    }
   ],
   "source": [
    "data_file = 'Data/USDA_Data_v2.0.xlsx'\n",
    "data_xl_sheets = pd.ExcelFile(data_file).sheet_names\n",
    "corn_df = pd.read_excel(data_file, sheet_name='Corn')[['Year','State','County','Data Item', 'Value']]\n",
    "barley_df = pd.read_excel(data_file, sheet_name='Barley')[['Year','State','County','Data Item','Value']]\n",
    "peanuts_df = pd.read_excel(data_file, sheet_name='Peanuts')[['Year','State','County','Data Item','Value']]\n",
    "soybean_df = pd.read_excel(data_file, sheet_name='Soybean')[['Year','State','County','Data Item','Value']]\n",
    "price_df = pd.read_excel(data_file, sheet_name='Price Received')[['Year','State','Commodity','Value']]\n",
    "# #importing unemployment rate\n",
    "# unemployment_df = pd.read_excel(data_file, sheet_name='US UnemploymentRate')\n",
    "# #setting case to upper, easier to merge with rest of the data.\n",
    "# unemployment_df['State']=unemployment_df['State'].str.upper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though the data we got downloaded from USDA website was as County level, it still seem to have another level of detail in it. After checking the raw data the level below County is 'Ag District'. We can ignore the column but expect duplicate rows for each County for a Year. This needs to be aggregated to County level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corn_df[corn_df.duplicated(['Year','State','County','Data Item'],keep=False)].sort_values(['Year','State','County','Data Item']).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For most of the crops we have aggregate data but for corn only the most widely produced variety has data about Acres Harvested, Production and Yield. Hence in case of Corn we will be using only the data of grain variety of corn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corn_df['Data Item'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tidy up\n",
    "Now we want to pivot the values in Data Item in thier own columns, so each row represnts only a obervation.\n",
    "\n",
    "But before that we need to shorten up values so that column names are short enough. To do that we do the following,\n",
    "1. Remove some of the redundent information like 'CORN, GRAIN'.\n",
    "2. Certain information from Data Item needs to be cleaned up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#CORN\n",
    "#Removing any combination of CORN GRAIN and preceding whitespaces\n",
    "corn_df.loc[:,'Data Item'] = (corn_df['Data Item'].str.replace(r'^CORN, GRAIN[\\s,][\\s-]','')).str.replace(r'^\\s','')\n",
    "\n",
    "#Removing anything after comma(,) as it just tells you the unit of the value.\n",
    "corn_df.loc[:,'Data Item'] = corn_df['Data Item'].str.split(',').str[0]\n",
    "\n",
    "#----------------------------------------------------------\n",
    "#BARLEY\n",
    "#Removing 'BARLEY -' and whitespaces\n",
    "barley_df.loc[:,'Data Item'] = (barley_df['Data Item'].str.replace(r'^BARLEY[\\s,][\\s-]','')).str.replace(r'^\\s','')\n",
    "\n",
    "#Removing anything after comma(,) as it just tells you the unit of the value.\n",
    "barley_df.loc[:,'Data Item'] = barley_df['Data Item'].str.split(',').str[0]\n",
    "\n",
    "#----------------------------------------------------------\n",
    "#PEANUTS\n",
    "#Removing 'PEANUTS -' and whitespaces\n",
    "peanuts_df.loc[:,'Data Item'] = (peanuts_df['Data Item'].str.replace(r'^PEANUTS[\\s,][\\s-]','')).str.replace(r'^\\s','')\n",
    "\n",
    "#Removing anything after comma(,) as it just tells you the unit of the value.\n",
    "peanuts_df.loc[:,'Data Item'] = peanuts_df['Data Item'].str.split(',').str[0]\n",
    "\n",
    "#----------------------------------------------------------\n",
    "#SOY BEANS\n",
    "#Removing 'SOYBEANS -' and whitespaces\n",
    "soybean_df.loc[:,'Data Item'] = (soybean_df['Data Item'].str.replace(r'^SOYBEANS[\\s,][\\s-]','')).str.replace(r'^\\s','')\n",
    "\n",
    "#Removing anything after comma(,) as it just tells you the unit of the value.\n",
    "soybean_df.loc[:,'Data Item'] = soybean_df['Data Item'].str.split(',').str[0]\n",
    "\n",
    "print(\"{A}\\n\\n{B}\\n\\n{C}\\n\\n{D}\".format(A=corn_df.head(),B=barley_df.head(),C=peanuts_df.head(),D=soybean_df.head()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pivoting the 'Data Item' as well as aggregating values at County level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#CORN\n",
    "pivoted_corn_df = corn_df.pivot_table(index=['Year','State','County'], columns='Data Item', values='Value',aggfunc='sum').reset_index()\n",
    "pivoted_corn_df.columns.name=None\n",
    "pivoted_corn_df.loc[:,'CROP']='CORN'\n",
    "\n",
    "#----------------------------------------------------------\n",
    "#BARLEY\n",
    "pivoted_barley_df = barley_df.pivot_table(index=['Year','State','County'], columns='Data Item', values='Value',aggfunc='sum').reset_index()\n",
    "pivoted_barley_df.columns.name=None\n",
    "pivoted_barley_df.loc[:,'CROP']='BARLEY'\n",
    "\n",
    "#----------------------------------------------------------\n",
    "#PEANUTS\n",
    "pivoted_peanuts_df = peanuts_df.pivot_table(index=['Year','State','County'], columns='Data Item', values='Value',aggfunc='sum').reset_index()\n",
    "pivoted_peanuts_df.columns.name=None\n",
    "pivoted_peanuts_df.loc[:,'CROP']='PEANUTS'\n",
    "\n",
    "#----------------------------------------------------------\n",
    "#SOY BEANS\n",
    "pivoted_soybean_df = soybean_df.pivot_table(index=['Year','State','County'], columns='Data Item', values='Value',aggfunc='sum').reset_index()\n",
    "pivoted_soybean_df.columns.name=None\n",
    "pivoted_soybean_df.loc[:,'CROP']='SOYBEANS'\n",
    "\n",
    "#----------------------------------------------------------\n",
    "#PRICE\n",
    "pivoted_price_df=price_df.copy()\n",
    "pivoted_price_df.columns = ['Year','State','CROP','PRICE']\n",
    "#A little clean up\n",
    "pivoted_price_df['PRICE'] = pivoted_price_df['PRICE'].replace(\" (NA)\",np.NaN)\n",
    "\n",
    "print(\"{A}\\n\\n{B}\\n\\n{C}\\n\\n{D}\\n\\n{E}\".format(A=corn_df.head(),B=barley_df.head(),C=peanuts_df.head(),D=soybean_df.head(),E=pivoted_price_df.head()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardizing all values to same unit Production in pounds (lbs) and price in \\$/pound(lb) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The unit of measurement of some crops (CORN, BARLEY and SOYBEAN) is in BU, while peanuts are in lb. \n",
    "# According below information even the number of pounds(lb) in an BU, varies from crop to crop. Since we will convert all the production in lb.\n",
    "# https://grains.org/markets-tools-data/tools/converting-grain-units/\n",
    "\n",
    "pivoted_corn_df.loc[:,'PRODUCTION'] = pivoted_corn_df['PRODUCTION']*56\n",
    "pivoted_barley_df.loc[:,'PRODUCTION'] = pivoted_barley_df['PRODUCTION']*48\n",
    "pivoted_soybean_df.loc[:,'PRODUCTION'] = pivoted_soybean_df['PRODUCTION']*60\n",
    "\n",
    "#Converting price from $/BU to $/lb\n",
    "pivoted_price_df.loc[pivoted_price_df['CROP']=='CORN','PRICE']/=56\n",
    "pivoted_price_df.loc[pivoted_price_df['CROP']=='BARLEY','PRICE']/=48\n",
    "pivoted_price_df.loc[pivoted_price_df['CROP']=='SOYBEANS','PRICE']/=60\n",
    "\n",
    "#Filling nulls\n",
    "pivoted_price_df.loc[:,'PRICE'] =pivoted_price_df.groupby(['Year','CROP'])['PRICE'].transform(lambda x: x.fillna(x.mean()))\n",
    "pivoted_price_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combining all crops into single dataframe\n",
    "crop_df = pd.concat([pivoted_corn_df,pivoted_barley_df,pivoted_peanuts_df,pivoted_soybean_df]).reset_index().drop(columns='index')\n",
    "\n",
    "#Recalculating Yield, as Sum is not an appropriate aggregation for it.\n",
    "crop_df.loc[:,'YIELD']=crop_df['PRODUCTION']/crop_df['ACRES HARVESTED']\n",
    "\n",
    "#Adding prices\n",
    "crop_df = pd.merge(crop_df, pivoted_price_df, how='left')\n",
    "\n",
    "#Rearranging columns\n",
    "crop_df = crop_df[['CROP','Year','State','County','ACRES HARVESTED','PRODUCTION','YIELD','PRICE']]\n",
    "crop_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking for any odd values in these columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_ch = ['`','~','!','@','#','$','%','^','&','*','(',')','_','-','+','=','{','[','}','}','|','\\\\',':',';','\"',\"'\",'<',',','>','.','?','/']\n",
    "esc_lst =[re.escape(s) for s in sp_ch]\n",
    "pattern = '|'.join(esc_lst)\n",
    "print('County:',crop_df[crop_df['County'].str.contains(pattern, case=False)]['County'].unique())\n",
    "print('State:',crop_df[crop_df['State'].str.contains(pattern, case=False)]['State'].unique())\n",
    "print('Year:',crop_df[(crop_df['Year']<1984) | (crop_df['Year']>2018)]['Year'].unique())\n",
    "print('Negative Values:',crop_df[(crop_df<0).any(1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There aren't any odd values in any of the columns but one value that can be of concern during modeling is \"OTHER (COMBINED) COUNTIES\". This is because mixture of combined counties changes for each year and crop. The range of values for Area, Production and Yield is so large that it will throw off the model for this County value. Hence we will drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(crop_df[crop_df['County']=='OTHER (COMBINED) COUNTIES'].groupby('CROP').describe().loc[:,(slice(None),['max','min'])])\n",
    "filtered_crop_df = crop_df[~(crop_df['County']=='OTHER (COMBINED) COUNTIES')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like all of the columns have valid data types that corresponds to the value expected in the column.\n",
    "But looking for the non-nulls in the key columns ACRES HARVESTED,PRODUCTION and YIELD have good population except for few nulls. Which we will deal with soon. For rest of the columns where non-null values that are not even 20%, we will have to ignore that data (drop columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(filtered_crop_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking above Nulls are in any of the numerical columns.\n",
    "# Dropping nulls if Harvested Acres or Production is missing, as these are important features. \n",
    "# Using statistical method to get approximate values may skew the model, hence dropping these rows is a better option.\n",
    "temp_crop_df = filtered_crop_df.dropna(axis=0,subset=['ACRES HARVESTED','PRODUCTION'])\n",
    "\n",
    "#Looking for pattern in the remaining nulls. Looks like the price is missing for Barley for the year 2018.\n",
    "# We can drop these too. Few values \n",
    "print('Pattern for Nulls: {}'.format(pd.unique(temp_crop_df[temp_crop_df.isnull().any(axis=1)][['CROP','Year']].values.ravel('K'))))\n",
    "print('What are other values does the above pattern have for Price: {}'.\\\n",
    "      format(temp_crop_df.loc[(temp_crop_df['CROP']=='BARLEY') & (temp_crop_df['Year']==2018),:]['PRICE'].unique()))\n",
    "\n",
    "clean_crop_df = temp_crop_df.dropna(axis=0,subset=['PRICE'])\n",
    "print('Any more nulls?\\n {}'.format(clean_crop_df.isnull().any()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking for duplicates by Crop, Year, State and County but there are none."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_crop_df[clean_crop_df.duplicated(['CROP','Year','State','County'],keep=False)]\n",
    "clean_crop_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of all the columns looks normally distributed. Although Acres Harvested and Production is denser towards the 0, which just mean there are more smaller farms. Hence no further action is required here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# #clean_crop_df[['ACRES HARVESTED','PRODUCTION','YIELD','PRICE']].hist(figsize=(15,25),layout=(4,1),bins=100, column=clean_crop_df['CROP'])\n",
    "# sns.distplot(clean_crop_df, kde=False, rug=True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Enrichment\n",
    "### Unemployment Rate\n",
    "This dataset is from Bureau of Labor Statistics. The downloaded excel workbook consisted of state wise unemployment for every month since Jan 1976 to Nov 2018. The set was split by state with each state in a separate sheet, hence needed to be combined into one large set. And since the crop statistics is on yearly basis, this set needs to be aggregated to yearly numbers.\n",
    "\n",
    "Once the set is combined and aggregated we write it into another excel file and later manually added as a seperate sheet to rest of the data workbook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xl_file = 'Data/DataFinder-20190114232211.xlsx'\n",
    "xl_sheets = pd.ExcelFile(xl_file).sheet_names\n",
    "unemployment_df = pd.DataFrame({})\n",
    "\n",
    "def read_unemployment_sheet(sheet):\n",
    "    #Reading one sheet from excel, while only selecting first 5 columns as rest of the columns are empty. \n",
    "    df = pd.read_excel(xl_file, sheet_name=sheet).iloc[:,:5]\n",
    "    \n",
    "    #Assigning state name, that's located at 7th row of 2nd column, to seperate column.\n",
    "    state_name = df.iloc[6,1]\n",
    "    #print(state_name)\n",
    "    df['Unnamed: 4']=state_name\n",
    "    \n",
    "    #Filtering out all rows above the actual data.\n",
    "    df = df.iloc[11:,:]\n",
    "    \n",
    "    #Renaming columns\n",
    "    df.columns = ['Year','Period','Month','Unemployment Rate','State']\n",
    "    \n",
    "    #Type casting to make sure 'Unemployment Rate' is numeric before taking yearly averages\n",
    "    df['Unemployment Rate']=pd.to_numeric(df['Unemployment Rate'])\n",
    "    \n",
    "    #Takes mean 'Unemployment Rate' across year.\n",
    "    df['Yearly Unemployment Rate'] = df.groupby('Year')['Unemployment Rate'].transform('mean')\n",
    "    \n",
    "    #returns selected columns and dropping duplicates.\n",
    "    return df[['State','Year','Yearly Unemployment Rate']].drop_duplicates()\n",
    "    \n",
    "unemployment_df = unemployment_df.append(map(read_unemployment_sheet,xl_sheets))\n",
    "unemployment_df['State']=unemployment_df['State'].str.upper()\n",
    "unemployment_df['Year']=unemployment_df['Year'].astype('int64')\n",
    "print(unemployment_df.head())\n",
    "# writer = pd.ExcelWriter('Data/US_Historical_UnemploymentRate_ByState.xlsx')\n",
    "# xl_df.to_excel(writer,'US UnemploymentRate', index=False)\n",
    "# writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining Unemployment data with yield."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unemp_crop_df = pd.merge(clean_crop_df,unemployment_df,how='left')\n",
    "unemp_crop_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weather data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exporting to Excel file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corp_writer = pd.ExcelWriter('Data/Crop_Pivoted.xlsx')\n",
    "unemp_crop_df.to_excel(corp_writer,'Crop Data', index=False)\n",
    "corp_writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
